 
 
 
 
 Instead of having an encoder analyse input tokens sequentially - we have a vector where each row represents a token. 
so the number of rows = number of tokens
 Now tokens are words and don't mean much to a machine so what this row needs to contain is a meaningful representation from the model's perspective. These meaningful representiations are called embeddings.